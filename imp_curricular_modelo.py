# -*- coding: utf-8 -*-
"""imp_curricular_modelo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SSlDVpVuVjp_gxKFGAwvQKUMt_CNCz_O

# Paquetes e importaciones y funciones
"""

import pandas as pd
import numpy as np
from datetime import datetime
from datetime import timedelta
from google.colab import sheets

# prompt: crear esta función # Tabla de frecuencias

def tabla_frecuencias(df, var):
  """
  Esta función calcula y muestra una tabla de frecuencias y porcentajes para una variable dada en un DataFrame.

  Args:
    df: El DataFrame que contiene los datos.
    var: El nombre de la variable para la cual se calcularán las frecuencias.

  Returns:
    Un DataFrame con las frecuencias y porcentajes.
  """


  # Calcula las frecuencias
  frecuencias = df[var].value_counts(dropna=False)

  # Calcula los porcentajes
  total = frecuencias.sum()
  porcentajes = (frecuencias / total) * 100

  # sumar otra variable
  #sum_aux = df.groupby(var)[aux].sum()
  # agregado variable
 # variable = df[var]

  # Crea un DataFrame con las frecuencias y porcentajes
  tabla = pd.DataFrame({
            # var:df[var], # agregado
      'Frecuencia': frecuencias,
      'Porcentaje': porcentajes,
            #'aux': sum_aux

  })

  #return print(tabla)
  return tabla

# Función resumen por variable sumando el número de alumnos

def resum_var(bd, var1, var2):
  """
  Esta función calcula y muestra un resumen de la suma de una variable (var2) agrupada por otra variable (var1) en un DataFrame (bd).

  Args:
    bd: El DataFrame que contiene los datos.
    var1: El nombre de la variable para agrupar.
    var2: El nombre de la variable para sumar.

  Returns:
    Un DataFrame con el resumen.
  """
  tabla = bd.groupby(var1)[var2].sum().reset_index()
  total = tabla[var2].sum()
  tabla['Porcentaje'] = (tabla[var2] / total) * 100

  # Agrega una fila con el total
  tabla_total = pd.DataFrame({var1: ['Total'], var2: [total], 'Porcentaje': [100]})
  tabla = pd.concat([tabla, tabla_total], ignore_index=True)

  return print(tabla)

# Ejemplo de uso
#resum_var(dfmat, 'COD_DEPE', 'N_ALU')

# prompt: crear una función con # prompt: HISTOGRAMA dfmat2 'N_ALU' CON COLUMNAS MÁS ANGOSTAS MOSTRAR MEDIA Y MEDIANA Y DESVEST
# import matplotlib.pyplot as plt
# import seaborn as sns
# var= 'N_ALU'
# # Calcula la media, mediana y desviación estándar
# media = dfmat2[var].mean()
# mediana = dfmat2[var].median()
# desvest = dfmat2[var].std()
# # Crea el histograma
# plt.figure(figsize=(10, 6))  # Ajusta el tamaño de la

import matplotlib.pyplot as plt
import seaborn as sns

def histograma_resumen(df, var, bins_adj):
  """
  Esta función crea un histograma para una variable dada en un DataFrame,
  muestra la media, mediana y desviación estándar, y ajusta el ancho de las columnas.

  Args:
    df: El DataFrame que contiene los datos.
    var: El nombre de la variable para la cual se creará el histograma.
  """
  # Calcula la media, mediana y desviación estándar
  media = df[var].mean()
  mediana = df[var].median()
  desvest = df[var].std()

  # Crea el histograma
  plt.figure(figsize=(10, 6))
  sns.histplot(df[var], kde=False, bins=bins_adj)  # Ajusta el número de bins según sea necesario

  # Agrega líneas verticales para la media, mediana y desviación estándar
  plt.axvline(media, color='red', linestyle='dashed', linewidth=2, label=f'Media: {media:.2f}')
  #plt.axvline(mediana, color='green', linestyle='dashed', linewidth=2, label=f'Mediana: {mediana:.2f}')
  plt.axvline(desvest, color='None', linestyle='None', linewidth=2, label=f'Desv. Est.: {desvest:.2f}')

  plt.axvline(np.percentile(df[var], 25), color='orange', linestyle='dotted', linewidth=2, label=f'P25.: {np.percentile(df[var], 25):.2f}')
  plt.axvline(np.percentile(df[var], 50), color='green', linestyle='dotted', linewidth=2, label=f'P50.: {np.percentile(df[var], 50):.2f}')
  plt.axvline(np.percentile(df[var], 75), color='orange', linestyle='dotted', linewidth=2, label=f'P75.: {np.percentile(df[var], 75):.2f}')
  plt.axvline(np.percentile(df[var], 90), color='orange', linestyle='dotted', linewidth=2, label=f'P90.: {np.percentile(df[var], 90):.2f}')
  plt.axvline(np.percentile(df[var], 95), color='orange', linestyle='dotted', linewidth=2, label=f'P95.: {np.percentile(df[var], 95):.2f}')

  #plt.axvline(media + desvest, color='blue', linestyle='dotted', linewidth=2)
  #plt.axvline(media - desvest, color='blue', linestyle='dotted', linewidth=2)

  # Configura el título y etiquetas
  plt.title('Histograma de ' + var)
  plt.xlabel(var)
  plt.ylabel('Frecuencia')
  plt.legend()

  # Muestra el gráfico
  plt.show()

# Ejemplo de uso
# histograma_resumen(dfmat2, 'N_ALU')

import matplotlib.pyplot as plt
import seaborn as sns

def histograma_resumen_0(df, var, bins_adj):
  """
  Esta función crea un histograma para una variable dada en un DataFrame,
  muestra la media, mediana y desviación estándar, y ajusta el ancho de las columnas.

  Args:
    df: El DataFrame que contiene los datos.
    var: El nombre de la variable para la cual se creará el histograma.
  """
  df = df[df[var] != 0]  # Filtra los casos en que def_vig = 0

  # Calcula la media, mediana y desviación estándar
  media = df[var].mean()
  mediana = df[var].median()
  desvest = df[var].std()

  # Crea el histograma
  plt.figure(figsize=(10, 6))
  sns.histplot(df[var], kde=False, bins=bins_adj)  # Ajusta el número de bins según sea necesario

  # Agrega líneas verticales para la media, mediana y desviación estándar
  plt.axvline(media, color='red', linestyle='dashed', linewidth=2, label=f'Media: {media:.2f}')
  #plt.axvline(mediana, color='green', linestyle='dashed', linewidth=2, label=f'Mediana: {mediana:.2f}')
  plt.axvline(desvest, color='None', linestyle='None', linewidth=2, label=f'Desv. Est.: {desvest:.2f}')

  plt.axvline(np.percentile(df[var], 25), color='orange', linestyle='dotted', linewidth=2, label=f'P25.: {np.percentile(df[var], 25):.2f}')
  plt.axvline(np.percentile(df[var], 50), color='green', linestyle='dotted', linewidth=2, label=f'P50.: {np.percentile(df[var], 50):.2f}')
  plt.axvline(np.percentile(df[var], 75), color='orange', linestyle='dotted', linewidth=2, label=f'P75.: {np.percentile(df[var], 75):.2f}')
  plt.axvline(np.percentile(df[var], 90), color='orange', linestyle='dotted', linewidth=2, label=f'P90.: {np.percentile(df[var], 90):.2f}')
  plt.axvline(np.percentile(df[var], 95), color='orange', linestyle='dotted', linewidth=2, label=f'P95.: {np.percentile(df[var], 95):.2f}')

  #plt.axvline(media + desvest, color='blue', linestyle='dotted', linewidth=2)
  #plt.axvline(media - desvest, color='blue', linestyle='dotted', linewidth=2)

  # Configura el título y etiquetas
  plt.title('Histograma de ' + var)
  plt.xlabel(var)
  plt.ylabel('Frecuencia')
  plt.legend()

  # Muestra el gráfico
  plt.show()

# Ejemplo de uso
# histograma_resumen(dfmat2, 'N_ALU')

# prompt: crear una función con df2_agrupado = df2_1.groupby('SUBSECTOR1_lab')['caso'].sum().reset_index()
# df2_agrupado

def agrupa_cuenta(df, var, caso):
  """
  Esta función agrupa un DataFrame por una columna y suma los valores de otra columna.

  Args:
    df: El DataFrame a agrupar.
    columna_agrupacion: El nombre de la columna por la cual agrupar.
    columna_suma: El nombre de la columna cuyos valores se van a sumar.

  Returns:
    Un DataFrame con la suma de los valores agrupados.
  """
  df_agrupado = df.groupby(var)[caso].sum().reset_index()
  df_agrupado = df_agrupado.sort_values(by=caso, ascending=False)
  total = df_agrupado[caso].sum()
  df_agrupado['Porcentaje'] = (df_agrupado[caso] / total) * 100
  sheet = sheets.InteractiveSheet(df=df_agrupado)

  return sheet


  #return df_agrupado

# prompt: crear una función con df2_agrupado = df2_1.groupby('SUBSECTOR1_lab')['caso'].sum().reset_index()
# # df2_agrupado y ordenar por la variable caso de mayor a menor y agregar el porcentaje

def agrupa_cuenta_ordenada(df, var, caso):
  """
  Esta función agrupa un DataFrame por una columna y suma los valores de otra columna,
  ordenando los resultados de mayor a menor y agregando el porcentaje.

  Args:
    df: El DataFrame a agrupar.
    var: El nombre de la columna por la cual agrupar.
    caso: El nombre de la columna cuyos valores se van a sumar.

  Returns:
    Un DataFrame con la suma de los valores agrupados, ordenado y con porcentajes.
  """
  df_agrupado = df.groupby(var)[caso].sum().reset_index()
  df_agrupado = df_agrupado.sort_values(by=caso, ascending=False)
  total = df_agrupado[caso].sum()
  df_agrupado['Porcentaje'] = (df_agrupado[caso] / total) * 100
  return df_agrupado

# prompt: conectar con google drive

from google.colab import drive
drive.mount('/content/drive')

"""# Importar archivos"""

from google.colab import drive
drive.mount('/content/drive')

# dda1_merged.csv es la base que cruza oferta y demanda por RBD, nivel_sector_lab, Niv_Plan_lab y Asig

dir='/content/drive/MyDrive/analisis_datos/imp_curricular/Insumos/dda1_merged.csv'


df = pd.read_csv(dir,
                 sep=';',
                 encoding='iso-8859-1',
                 #on_bad_lines='skip'
                 )



df.info()

#df.head()

# Crear una tabla con los campos, número de columna y tipo de variable
tabla_info = pd.DataFrame({
    'Campo': df.columns,
    'Número de Columna': range(1, len(df.columns) + 1),
    'Tipo de Variable': df.dtypes
})

# Mostrar la tabla
#tabla_info

#df.head()

df =df.assign(dup_rbd=df[['RBD']].duplicated())
df ['dup_rbd'].value_counts(dropna=False)

# prompt: rellenar con 0 las celdas vacías en 's_h_esc_vig' 'def_vig' 'sup_vig' 'h_gen1'] 'horas_pl2_pro'

# Fill NaN values in specified columns with 0
columns_to_fill = ['s_h_esc_vig',
                   's_h_esc_min',
                   's_h_esc_max',
                   'def_vig',
                   'def_vig_min',
                   'def_vig_max',
                   'sup_vig',
                   'sup_vig_min',
                   'sup_vig_max',
                   #'h_gen1',
                   'horas_pl2_pro',
                   'dif_h',
                   'dif_h_min',
                   'dif_h_max',
                   ]

df[columns_to_fill] = df[columns_to_fill].fillna(0)

"""Filtro considera solo rbd con matrícula"""

# prompt: crea un listado de RBD unicos de la base df, cuando df['dfmat9']==dfmat9

filtered_df = df[df['dfmat9'] == 'dfmat9']
unique_rbd_list = filtered_df['RBD'].unique().tolist()
print(len(unique_rbd_list))

# prompt: en df keep los RBD que estan dentro de unique_rbd_list

# Keep only rows where 'RBD' is in unique_rbd_list
df= df[df['RBD'].isin(unique_rbd_list)]

df ['dup_rbd'].value_counts(dropna=False)

# prompt: en df no considerar los casos donde Asig = "NA_SC"

# Assuming 'Asig' is the column you want to filter
df = df[df['Asig'] != "NA_SC"]

"""Considera de 1B a 2M, excluye 3M y 4M"""

df= df[df['Niv_Plan_lab'] != '3M-4M']

df['Niv_Plan_lab'].value_counts(dropna=False)

"""Importa directorio establecimiento"""

dir='/content/drive/MyDrive/analisis_datos/imp_curricular/Insumos/20240514_Directorio_preliminar_EE_2024_20240430_PUBL.csv'


df_direc = pd.read_csv(dir,
                 sep=';',
                 encoding='iso-8859-1',
                 #on_bad_lines='skip'
                 )



df_direc.info()

"""Importa matriz de horas para cálculo de jornadas"""

dir='/content/drive/MyDrive/analisis_datos/imp_curricular/Insumos/dfdoc2.csv'


dfdoc2 = pd.read_csv(dir,
                 sep=';',
                 encoding='iso-8859-1',
                 #on_bad_lines='skip'
                 )

dfdoc2['marca'] = "dfdoc2"
dfdoc2.info()
dfdoc2['marca'].value_counts(dropna=False)

"""# Casos a modelar"""

dda1_merged= df
dda1_merged.info()

# prompt: generar un listado con los rbd unicos de dda1_merged y contarlos

# Obtener los RBD únicos
rbd_unicos = dda1_merged['RBD'].unique()

# Imprimir el listado de RBD únicos
print("Listado de RBD únicos:")
print(rbd_unicos)

# Contar la cantidad de RBD únicos
cantidad_rbd_unicos = len(rbd_unicos)

# Imprimir la cantidad de RBD únicos
print("\nCantidad de RBD únicos:", cantidad_rbd_unicos)

# prompt: crear un data frame con los rbd_unicos

# Crear un DataFrame con los RBD únicos
df_rbd_unicos = pd.DataFrame({'RBD': rbd_unicos})

# Imprimir el DataFrame
df_rbd_unicos.info()

# prompt: en df_rbd_unicos  crea una variable grupo que agrupe cada 50 RBD

# Assuming df_rbd_unicos is already created as in your provided code

# Calculate the group number for each RBD
df_rbd_unicos['grupo'] = (df_rbd_unicos.index // 10) + 1

# Display the updated DataFrame
df_rbd_unicos.info()

# prompt: cuántos grupos tiene df_rbd_unicos['grupo']
print(df_rbd_unicos['grupo'].nunique())

# prompt: cruzar  dda1_merged y df_rbd_unicos por RBD

# Merge the two dataframes based on the 'RBD' column
dda1_merged = pd.merge(dda1_merged, df_rbd_unicos, on='RBD', how='left')

# Now 'merged_df' contains the combined data, with 'grupo' information added from 'df_rbd_unicos'
dda1_merged.info()

dda1_merged ['dup_rbd7'].value_counts(dropna=False)

dda1_merged['grupo'].unique()

"""Cruce con datos del directorio"""

# MERGE

dda1_merged = pd.merge(dda1_merged, df_direc[['RBD',
                                      'COD_REG_RBD',
                                      'RURAL_RBD',
                                      #'Niv_Plan_lab',
                                      #'horas_pl2_pro'
                                              ]],
                                      on=['RBD',
                                          #'Asig',
                                          #'nivel_sector_lab',
                                          #'Niv_Plan_lab'
                                          ],
                                      how='left'
                                      )


# RURAL_RBD
RURAL_RBD_dic = {  0 : 'Urbano',  1 : 'Rural',}
dda1_merged['RURAL_RBD_lab'] = dda1_merged.apply(lambda x:RURAL_RBD_dic .get(x['RURAL_RBD'], 'Unknown'), axis=1)

dda1_merged.info()
dda1_merged ['RURAL_RBD_lab'].value_counts(dropna=False)

"""Cruce con media_h para calcular jornadas docentes"""

# MERGE

dda1_merged = pd.merge(dda1_merged, dfdoc2[[
                                              #'COD_REG_RBD',
                                              #'RURAL_RBD',
                                              'RURAL_RBD_lab',
                                              'Asig',
                                              'marca',
                                              'nivel_sector_lab',
                                              'media_h',
                                              #'Niv_Plan_lab',
                                              #'horas_pl2_pro',
                                                      ]],
                                              on=['Asig',
                                                  #'COD_REG_RBD',
                                                  'nivel_sector_lab',
                                                  'RURAL_RBD_lab',
                                                  #'Niv_Plan_lab'
                                                  ],
                                              how='left'
                                              )


dda1_merged.info()
dda1_merged ['marca'].value_counts(dropna=False)
dda1_merged ['media_h'].describe()

"""Calcula horas promedio para los casos vacíos"""

art = dfdoc2[dfdoc2['Asig'] == 'Art']['media_h'].mean()
art_vis = dfdoc2[dfdoc2['Asig'] == 'Art_Vis']['media_h'].mean()
art_esc = dfdoc2[dfdoc2['Asig'] == 'Art_Esc']['media_h'].mean()
art_mus = dfdoc2[dfdoc2['Asig'] == 'Art_Mus']['media_h'].mean()

ori= dfdoc2[dfdoc2['Asig'] == 'Ori']['media_h'].mean()
tec= dfdoc2[dfdoc2['Asig'] == 'Tec']['media_h'].mean()
relig= dfdoc2[dfdoc2['Asig'] == 'Relig']['media_h'].mean()
hld = dfdoc2[dfdoc2['Asig'] == 'HLD']['media_h'].mean()

ing = dfdoc2[dfdoc2['Asig'] == 'Ing']['media_h'].mean()
mat = dfdoc2[dfdoc2['Asig'] == 'Mat']['media_h'].mean()
lyl = dfdoc2[dfdoc2['Asig'] == 'LyL']['media_h'].mean()
his = dfdoc2[dfdoc2['Asig'] == 'His']['media_h'].mean()
cn = dfdoc2[dfdoc2['Asig'] == 'CN']['media_h'].mean()
ef= dfdoc2[dfdoc2['Asig'] == 'Ed_Fis']['media_h'].mean()

j_doc=25.14

dda1_merged ['media_h']= dda1_merged ['media_h'].fillna(0)

dda1_merged.loc[(dda1_merged['Asig'] == 'Art_Esc') & (dda1_merged['media_h'] == 0), 'media_h'] = art_esc
dda1_merged.loc[(dda1_merged['Asig'] == 'Art_Vis') & (dda1_merged['media_h'] == 0), 'media_h'] = art_vis
dda1_merged.loc[(dda1_merged['Asig'] == 'Art') & (dda1_merged['media_h'] == 0), 'media_h'] = art
dda1_merged.loc[(dda1_merged['Asig'] == 'Art_Mus') & (dda1_merged['media_h'] == 0), 'media_h'] = art_mus

dda1_merged.loc[(dda1_merged['Asig'] == 'Ori') & (dda1_merged['media_h'] == 0), 'media_h'] = ori
dda1_merged.loc[(dda1_merged['Asig'] == 'Relig') & (dda1_merged['media_h'] == 0), 'media_h'] = relig
dda1_merged.loc[(dda1_merged['Asig'] == 'HLD') & (dda1_merged['media_h'] == 0), 'media_h'] = hld
dda1_merged.loc[(dda1_merged['Asig'] == 'Tec') & (dda1_merged['media_h'] == 0), 'media_h'] = tec

dda1_merged.loc[(dda1_merged['Asig'] == 'Ing') & (dda1_merged['media_h'] == 0), 'media_h'] = ing
dda1_merged.loc[(dda1_merged['Asig'] == 'Mat') & (dda1_merged['media_h'] == 0), 'media_h'] = mat
dda1_merged.loc[(dda1_merged['Asig'] == 'LyL') & (dda1_merged['media_h'] == 0), 'media_h'] = lyl
dda1_merged.loc[(dda1_merged['Asig'] == 'His') & (dda1_merged['media_h'] == 0), 'media_h'] = his
dda1_merged.loc[(dda1_merged['Asig'] == 'CN') & (dda1_merged['media_h'] == 0), 'media_h'] = cn
dda1_merged.loc[(dda1_merged['Asig'] == 'Ed_Fis') & (dda1_merged['media_h'] == 0), 'media_h'] = ef

dda1_merged.loc[(dda1_merged['Asig'] == 'general') & (dda1_merged['media_h'] == 0), 'media_h'] = j_doc

dda1_merged ['media_h'].describe()

# prompt: en dda1_merged ['media_h'] mostrar el porcentaje de 0 y la cantidad de casos

# Calculate the percentage of 0s and the count of cases for 'media_h'
zero_count = dda1_merged[dda1_merged['media_h'] == 0].shape[0]
total_count = dda1_merged.shape[0]
percentage_zero = (zero_count / total_count) * 100

print(f"Percentage of 0s in 'media_h': {percentage_zero:.2f}%")
print(f"Number of cases with 0 in 'media_h': {zero_count}")
print(f"Total number of cases in 'media_h': {total_count}")

histograma_resumen(dda1_merged, 'media_h', 50)

dda1_merged[[ 'RBD',
              'Asig',
             'COD_REG_RBD',
              'nivel_sector_lab',
              'RURAL_RBD_lab',
              'media_h',

             ]]

# prompt: graficar dda1_merged ['media_h'] por asig como boxplot

plt.figure(figsize=(12, 6))
sns.boxplot(x='Asig', y='media_h', data=dda1_merged)
plt.title('Boxplot de media_h por Asignatura')
plt.xlabel('Asignatura')
plt.ylabel('Media de Horas')
plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability
plt.tight_layout()
plt.show()

"""# Modelamiento escenarios

Escenario base
"""

# tabla vacía para vaciar los datos

dda1_merged_cons=dda1_merged
dda1_merged_cons = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

number_list = list(range(1, 654)) # 159
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]

    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator

#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura dentro del nivel          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor

    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            (dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])
            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction

    #dda1_merged_1['h_cuadre_3'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura entre niveles          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] != nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            #(dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])

            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction


   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']
    dda1_merged_1['escenario'] = "base"

  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

dda1_merged_cons.info()

"""Escenario max"""

# tabla vacía para vaciar los datos

#dda1_merged_cons=dda1_merged
#dda1_merged_cons = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

#number_list = list(range(1, 2))
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]
    dda1_merged_1['dif_h']= dda1_merged_1['dif_h_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['s_h_esc_vig']= dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['s_h_esc_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['def_vig']= dda1_merged_1['def_vig_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['sup_vig']= dda1_merged_1['sup_vig_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator
######################################################### MODELO EXPERIMENTAL ARTES #################################################################################

# prompt: asignar horas sup_vig>0 de las Asig  "Art_mus" y "Art_vis" de un determinado RBD nivel_sector_lab, al def_vig de la asignatura Art para el mismo  RBD- nivel_sector_lab, 'Niv_Plan_lab'

    # ... (Your existing code)

        # New code to address the specific task:
    for index, row in dda1_merged_1.iterrows():
       if row['Asig'] in ["Art_Vis", "Art_Mus"] and row['sup_vig'] > 0:
            rbd_val = row['RBD']
            nivel_sector_val = row['nivel_sector_lab']
            niv_plan_lab_val = row['Niv_Plan_lab']

            # Find corresponding 'Art' rows
            art_rows = dda1_merged_1[
                (dda1_merged_1['RBD'] == rbd_val) &
                (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
                (dda1_merged_1['Niv_Plan_lab'] == niv_plan_lab_val) &
                (dda1_merged_1['Asig'] == 'Art') &
                (dda1_merged_1['def_vig'] < 0)  # Only consider rows with def_vig < 0
            ]

            for art_index, art_row in art_rows.iterrows():
                while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[art_index, 'def_vig'] < 0:
                    reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[art_index, 'def_vig'])
                    dda1_merged_1.loc[index, 'sup_vig'] -= reduction
                    dda1_merged_1.loc[art_index, 'def_vig'] += reduction

    # ... (Rest of your existing code)

#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura dentro del nivel          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor

    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            (dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])
            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction

    #dda1_merged_1['h_cuadre_3'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura entre niveles          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] != nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            #(dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])

            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction


   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']
    dda1_merged_1['escenario'] = "max"

  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

"""Escenario min"""

# tabla vacía para vaciar los datos

#dda1_merged_cons=dda1_merged
#dda1_merged_cons = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

#number_list = list(range(1, 2))
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]
    dda1_merged_1['dif_h']= dda1_merged_1['dif_h_min'] # *** ajuste para ocupar el mismo modelo pero con ajuste min ****
    dda1_merged_1['s_h_esc_vig']= dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['s_h_esc_min'] # *** ajuste para ocupar el mismo modelo pero con ajuste min ****
    dda1_merged_1['def_vig']= dda1_merged_1['def_vig_min'] # *** ajuste para ocupar el mismo modelo pero con ajuste min ****
    dda1_merged_1['sup_vig']= dda1_merged_1['sup_vig_min'] # *** ajuste para ocupar el mismo modelo pero con ajuste min ****
    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator

######################################################### MODELO EXPERIMENTAL ARTES #################################################################################

# prompt: asignar horas sup_vig>0 de las Asig  "Art_mus" y "Art_vis" de un determinado RBD nivel_sector_lab, al def_vig de la asignatura Art para el mismo  RBD- nivel_sector_lab, 'Niv_Plan_lab'

    # ... (Your existing code)

        # New code to address the specific task:
    for index, row in dda1_merged_1.iterrows():
       if row['Asig'] in ["Art_Vis", "Art_Mus"] and row['sup_vig'] > 0:
            rbd_val = row['RBD']
            nivel_sector_val = row['nivel_sector_lab']
            niv_plan_lab_val = row['Niv_Plan_lab']

            # Find corresponding 'Art' rows
            art_rows = dda1_merged_1[
                (dda1_merged_1['RBD'] == rbd_val) &
                (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
                (dda1_merged_1['Niv_Plan_lab'] == niv_plan_lab_val) &
                (dda1_merged_1['Asig'] == 'Art') &
                (dda1_merged_1['def_vig'] < 0)  # Only consider rows with def_vig < 0
            ]

            for art_index, art_row in art_rows.iterrows():
                while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[art_index, 'def_vig'] < 0:
                    reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[art_index, 'def_vig'])
                    dda1_merged_1.loc[index, 'sup_vig'] -= reduction
                    dda1_merged_1.loc[art_index, 'def_vig'] += reduction



    # ... (Rest of your existing code)
#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura dentro del nivel          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor

    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            (dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])
            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction

    #dda1_merged_1['h_cuadre_3'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura entre niveles          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] != nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            #(dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])

            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction


   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']
    dda1_merged_1['escenario'] = "min"

  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

dda1_merged_cons.info()
dda1_merged_cons ['escenario'].value_counts(dropna=False)

# prompt: rellenar con 0 los h_gen1 nulos

dda1_merged_cons['h_gen1'] = dda1_merged_cons['h_gen1'].fillna(0)

# Variable cuadre

dda1_merged_cons ['h_cuadre'] = dda1_merged_cons ['s_h_esc_vig'] + dda1_merged_cons ['def_vig'] + dda1_merged_cons ['sup_vig'] + dda1_merged_cons ['h_gen1'] - dda1_merged_cons ['horas_pl2_pro']

"""Jornadas docentes"""

# Jornadas docentes de déficit y superavit

j_doc=25.14

dda1_merged_cons ['doc_def_vig'] = dda1_merged_cons ['def_vig'] / dda1_merged_cons ['media_h']
dda1_merged_cons ['doc_sup_vig'] = dda1_merged_cons ['sup_vig'] / dda1_merged_cons ['media_h']

# marca rbd por RBD, Niv_Plan_lab, Asig

# duplicados
dda1_merged_cons  =dda1_merged_cons .assign(dup_rbd_10=dda1_merged_cons [['RBD', 'Asig', 'Niv_Plan_lab']].duplicated())
dda1_merged_cons ['dup_rbd_10'].value_counts(dropna=False)
# prompt: crear variable  rbd_1 donde la variable es 1 si 'dup_rbd'=false y 0 cuando 'dup_rbd'=

dda1_merged_cons['rbd_as_pe'] = np.where(dda1_merged_cons['dup_rbd_10'] == False, 1, 0)

# marca rbd por RBD, nivel_sector_lab, Asig

# duplicados
dda1_merged_cons  =dda1_merged_cons .assign(dup_rbd_11=dda1_merged_cons [['RBD', 'Asig', 'nivel_sector_lab']].duplicated())
dda1_merged_cons ['dup_rbd_11'].value_counts(dropna=False)
# prompt: crear variable  rbd_1 donde la variable es 1 si 'dup_rbd'=false y 0 cuando 'dup_rbd'=

dda1_merged_cons['rbd_as_ns'] = np.where(dda1_merged_cons['dup_rbd_11'] == False, 1, 0)

# establecimiento rbd con déficit ---> depende de la agrupación

dda1_merged_cons['def_rbd'] = np.where(dda1_merged_cons['def_vig'] < 0, 1, 0)

dda1_merged_cons= dda1_merged_cons.sort_values(by=['nivel_sector_lab', 'Niv_Plan_lab' , 'RBD', 'def_vig'], ascending=[True, True, True, False])
dda1_merged_cons.info()
dda1_merged_cons ['dup_rbd7'].value_counts(dropna=False)

"""exportar"""

# prompt: variable con el día y la hora actual gmt -3

from datetime import datetime, timedelta
# Obtener la hora actual en UTC
now_utc = datetime.utcnow()
# Calcular la diferencia horaria para GMT-3
gmt_minus_3 = timedelta(hours=-3)
# Aplicar la diferencia horaria a la hora actual en UTC
now_gmt_minus_3 = now_utc + gmt_minus_3
# Formatear la fecha y hora como una cadena
current_datetime_gmt_minus_3_str = now_gmt_minus_3.strftime("%Y-%m-%d %H:%M:%S")
# Imprimir la fecha y hora actual en GMT-3
print("Fecha y hora actual (GMT-3):", current_datetime_gmt_minus_3_str)
# Puedes usar current_datetime_gmt_minus_3_str en tu código
# Ejemplo de asignación a una variable:
fyh = current_datetime_gmt_minus_3_str



inicio = "/content/drive/MyDrive/analisis_datos/imp_curricular/salida/dda1_merged_1_"
fecha=fyh
fin = ".csv"
dir = inicio + fecha + fin
dir



dda1_merged_cons.to_csv(dir,
               sep=";",
               decimal=",",
               encoding='iso-8859-1')


dda1_merged_cons.to_csv("/content/drive/MyDrive/analisis_datos/imp_curricular/salida/dda1_merged_1.csv",
               sep=";",
               decimal=",",
               encoding='iso-8859-1')


dda1_merged_cons.to_csv("/content/drive/MyDrive/analisis_datos/imp_curricular/salida/dda1_merged_11.csv",
              sep=";",
               #decimal=",",
               encoding='iso-8859-1')

"""Salida para pruebas muestrales"""

#### EXPORTA RBD ####

dfrev=dda1_merged_cons[dda1_merged_cons['grupo'] == 1] # para esta salida de análisis se exporta solo el grupo indicado

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# BORRAR HOJA
  # Open the Google Sheet by its key
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc")
# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different
# Clear the contents of the worksheet
worksheet.clear()

# ESCRIBIR HOJA
from gspread_dataframe import set_with_dataframe
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc").sheet1
set_with_dataframe(sheet, dfrev)

"""# Tablas inteligentes"""

# dda1_merged.csv es la base que cruza oferta y demanda por RBD, nivel_sector_lab, Niv_Plan_lab y Asig

dir='/content/drive/MyDrive/analisis_datos/imp_curricular/salida/dda1_merged_11.csv'


dda1_merged_11 = pd.read_csv(dir,
                 sep=';',
                 encoding='iso-8859-1',
                 #on_bad_lines='skip'
                 )


dda1_merged_11.info()

dda1_merged_11.head()
dda1_merged_11 ['Asig'].value_counts(dropna=False)

"""Provisorio jornadas docentes disponibles"""

# jornadas docentes disponibles


dda1_merged_11 ['doc_disp'] = dda1_merged_11 ['horas_pl2_pro'] / dda1_merged_11 ['media_h']


# jornadas docentes educación general no distribuida

dda1_merged_11 ['doc_gen'] = dda1_merged_11 ['h_gen1'] / dda1_merged_11 ['media_h']

# prompt: agrupar dda1 por Asig sumando 's_h_esc_vig' 'horas_pl' y 'dif_h' hd

dda1_agrupado_11 = dda1_merged_11.groupby(['nivel_sector_lab',
                                           'Niv_Plan_lab',
                                           'Asig',
                                           'escenario',]).agg({
    's_h_esc_vig': 'sum',
    #'s_h_esc_max': 'sum',
    #'rbd_11': 'sum',
   #'rbd_1': 'sum',
    #'s_h_esc_min': 'sum',
    'horas_pl2_pro': 'sum',
    'dif_h': 'sum',
    'def_vig': 'sum',
    'sup_vig': 'sum',
    'doc_def_vig': 'sum',
    'doc_sup_vig': 'sum',
    #'h_gen': 'sum',
    'h_gen1': 'sum',
     'h_cuadre': 'sum',
    'doc_disp':'sum',
    'doc_gen':'sum',
    #'h_gen2': 'sum',
    #'hd_def_rbd': 'sum'
}).reset_index()

dda1_agrupado_11.info()

dda1_agrupado_11.head()

dda1_agrupado_11.to_csv("/content/drive/MyDrive/analisis_datos/imp_curricular/salida/dda1_agrupado_11.csv",
               sep=";",
               decimal=",",
               encoding='iso-8859-1')

"""provisorio: jornadas docentes disponibles"""

#### EXPORTA RBD ####

dfrev=dda1_agrupado_11 #[(dda1_agrupado['RBD'] == 25342)]

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# BORRAR HOJA
  # Open the Google Sheet by its key
sheet = gc.open_by_key("1l_oEGH1zPmwewiGVfMg4Ln9waNmxmW17uI5wdst7yZs")
# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different
# Clear the contents of the worksheet
worksheet.clear()

# ESCRIBIR HOJA
from gspread_dataframe import set_with_dataframe
sheet = gc.open_by_key("1l_oEGH1zPmwewiGVfMg4Ln9waNmxmW17uI5wdst7yZs").sheet1
set_with_dataframe(sheet, dfrev)

"""# Nueva sección"""

#### EXPORTA RBD ####

dfrev=dda1_merged_cons #[(dda1_agrupado['RBD'] == 25342)]

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# BORRAR HOJA
  # Open the Google Sheet by its key
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc")
# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different
# Clear the contents of the worksheet
worksheet.clear()

# ESCRIBIR HOJA
from gspread_dataframe import set_with_dataframe
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc").sheet1
set_with_dataframe(sheet, dfrev)

# New code to address the specific task:
    for index, row in dda1_merged_1.iterrows():
       if row['Asig'] in ["Art_mus", "Art_vis"] and row['sup_vig'] > 0:
            rbd_val = row['RBD']
            nivel_sector_val = row['nivel_sector_lab']
            niv_plan_lab_val = row['Niv_Plan_lab']

            # Find corresponding 'Art' rows
            art_rows = dda1_merged_1[
                (dda1_merged_1['RBD'] == rbd_val) &
                (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
                (dda1_merged_1['Niv_Plan_lab'] == niv_plan_lab_val) &
                (dda1_merged_1['Asig'] == 'Art') &
                (dda1_merged_1['def_vig'] < 0)  # Only consider rows with def_vig < 0
            ]

            for art_index, art_row in art_rows.iterrows():
                while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[art_index, 'def_vig'] < 0:
                    reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[art_index, 'def_vig'])
                    dda1_merged_1.loc[index, 'sup_vig'] -= reduction
                    dda1_merged_1.loc[art_index, 'def_vig'] += reduction

#### EXPORTA RBD ####

dfrev=dda1_merged_11 [['RBD',
                      'nivel_sector_lab',
                      'Niv_Plan_lab',
                      'Asig',
                      's_h_esc_vig',
                      'horas_pl2_pro',
                      'dif_h',
                      'def_vig',
                      'sup_vig',
                      'doc_def_vig',
                      'doc_sup_vig',
                      'h_gen1',
                      'escenario',
                       'h_cuadre',

]]

  #[(dda1_merged_11['RBD'] == 5)]

#[(dda1_agrupado['RBD'] == 25342)]

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# BORRAR HOJA
  # Open the Google Sheet by its key
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc")
# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different
# Clear the contents of the worksheet
worksheet.clear()

# ESCRIBIR HOJA
from gspread_dataframe import set_with_dataframe
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc").sheet1
set_with_dataframe(sheet, dfrev)

"""análisis superavit - hld"""

# prompt: En dda1_agrupado_11 crear una variable hld_def  con el valor s_h_esc_vig si   Asig=HLD.

# Create 'hld_def' in dda1_agrupado_11
dda1_merged_12=dda1_merged_11
dda1_merged_12['hld_def'] = np.where(dda1_merged_12['Asig'] == 'HLD', dda1_merged_12['s_h_esc_vig'], 0)
dda1_merged_12['sup_ante'] = np.where(dda1_merged_12['dif_h'] > 0, dda1_merged_12['dif_h'], 0)

dda1_merged_12=dda1_merged_12[['RBD',
                               'nivel_sector_lab',
                               'Niv_Plan_lab', 'Asig',
                               's_h_esc_vig',
                               'horas_pl2_pro',
                               'dif_h',
                               'hld_def',
                               'sup_ante'
                               ]]


dda1_agrupado_12 = dda1_merged_12.groupby(['RBD',
                                           'Niv_Plan_lab',
                                           #'nivel_sector_lab',
                                           #'Asig'

                                           ]).agg({
    's_h_esc_vig': 'sum',
    #'s_h_esc_max': 'sum',
    #'rbd_11': 'sum',
   #'rbd_1': 'sum',
    #'s_h_esc_min': 'sum',
    'horas_pl2_pro': 'sum',
    'dif_h': 'sum',
    #'def_vig': 'sum',
    #'sup_vig': 'sum',
    #'doc_def_vig': 'sum',
    #'doc_sup_vig': 'sum',
    #'h_gen': 'sum',
    #'h_gen1': 'sum',
    #'h_gen2': 'sum',
    #'hd_def_rbd': 'sum'
    'hld_def': 'sum',
    'sup_ante': 'sum'
}).reset_index()

dda1_agrupado_12['exc_hld'] = dda1_agrupado_12['sup_ante'] - dda1_agrupado_12['hld_def']
dda1_agrupado_12['control_exc_hld'] = np.where(dda1_agrupado_12['exc_hld'] > 0, "excede", "no excede")

dda1_agrupado_12.info()
dda1_agrupado_12.head()
#dda1_agrupado_12['control_exc_hld'].value_counts(dropna=False)

tabla_frecuencias(dda1_agrupado_12, 'control_exc_hld')

"""Revisión de cuadre"""

# Revisa el cuadre de los establecimientos

dda1_merged_13=dda1_merged_11

#dda1_merged_12['hld_def'] = np.where(dda1_merged_12['Asig'] == 'HLD', dda1_merged_12['s_h_esc_vig'], 0)
#dda1_merged_12['sup_ante'] = np.where(dda1_merged_12['dif_h'] > 0, dda1_merged_12['dif_h'], 0)

#dda1_merged_13=dda1_merged_13[['RBD',
#                               'nivel_sector_lab',
#                               'Niv_Plan_lab', 'Asig',
#                               's_h_esc_vig',
#                               'horas_pl2_pro',
#                               'dif_h',
#                               'hld_def',
#                              'sup_ante',
#                               'h_cuadre'
#                               ]


dda1_agrupado_13 = dda1_merged_13.groupby(['RBD',
                                           #'Niv_Plan_lab',
                                           #'nivel_sector_lab',
                                           #'Asig'

                                           ]).agg({
    's_h_esc_vig': 'sum',
    #'s_h_esc_max': 'sum',
    #'rbd_11': 'sum',
   #'rbd_1': 'sum',
    #'s_h_esc_min': 'sum',
    'horas_pl2_pro': 'sum',
    'dif_h': 'sum',
    #'def_vig': 'sum',
    #'sup_vig': 'sum',
    #'doc_def_vig': 'sum',
    #'doc_sup_vig': 'sum',
    #'h_gen': 'sum',
    #'h_gen1': 'sum',
    #'h_gen2': 'sum',
    #'hd_def_rbd': 'sum'
    #'hld_def': 'sum',
    #'sup_ante': 'sum',
    'h_cuadre': 'sum'
}).reset_index()


dda1_agrupado_13['h_cuadre'] = dda1_agrupado_13['h_cuadre'].round(2)

#dda1_agrupado_12['exc_hld'] = dda1_agrupado_12['sup_ante'] - dda1_agrupado_12['hld_def']
dda1_agrupado_13['control_cuadre'] = np.where(dda1_agrupado_13['h_cuadre'] == 0, "esperado", "NA")
dda1_agrupado_13['control_cuadre'] = np.where(dda1_agrupado_13['h_cuadre'] > 0, "supera_horas", dda1_agrupado_13['control_cuadre'])
dda1_agrupado_13['control_cuadre'] = np.where(dda1_agrupado_13['h_cuadre'] < 0, "faltan_horas", dda1_agrupado_13['control_cuadre'])

#dda1_agrupado_12.info()
#dda1_agrupado_12.head()
#dda1_agrupado_12['control_exc_hld'].value_counts(dropna=False)

dda1_agrupado_13.head()
tabla_frecuencias(dda1_agrupado_13, 'control_cuadre')

#dda1_agrupado_13['control_cuadre'].value_counts(dropna=False)

# prompt: en dda1_agrupado_13 mostrar RBD donde control_cuadre="faltan_horas"

# Assuming dda1_agrupado_13 is already defined as in your provided code.

# Filter the DataFrame where 'control_cuadre' is equal to "faltan_horas"
faltan_horas_df = dda1_agrupado_13[dda1_agrupado_13['control_cuadre'] == "faltan_horas"]

# Print the 'RBD' column of the filtered DataFrame
faltan_horas_df[['RBD', 'h_cuadre', 'control_cuadre']]

"""# Modela escenarios

para pruebas base
"""

# tabla vacía para vaciar los datos

dda1_merged_cons=dda1_merged
dda1_merged_cons = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

number_list = list(range(1, 2))
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]

    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator

#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']

   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']
    dda1_merged_1['escenario'] = "base"

  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

"""para pruebas max"""

# tabla vacía para vaciar los datos

#dda1_merged_cons=dda1_merged
#dda1_merged_cons = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

#number_list = list(range(1, 2))
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]
    dda1_merged_1['dif_h']= dda1_merged_1['dif_h_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['s_h_esc_vig']= dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['s_h_esc_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['def_vig']= dda1_merged_1['def_vig_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    dda1_merged_1['sup_vig']= dda1_merged_1['sup_vig_max'] # *** ajuste para ocupar el mismo modelo pero con ajuste max ****
    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator

#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura dentro del nivel          ##########################################################


##################################################         Modelo horas excedentes misma asignatura entre niveles          ##########################################################


   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']
    dda1_merged_1['escenario'] = "max"

  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

"""Escenario max"""

dda1_merged.info()

dda1_merged_max=dda1_merged[['RBD',
                            'Asig',
                            'nivel_sector_lab',
                            'Niv_Plan_lab',
                            's_h_esc_vig',
                            's_h_esc_max',
                            'horas_pl2_pro',
                            'dif_h_max',
                            'def_vig_max',
                            'sup_vig_max',

                            ]]
dda1_merged_max.info()

dda1_merged_max.head()

# tabla vacía para vaciar los datos

dda1_merged_max=dda1_merged[ 'RBD',
                            'Asig',
                            'nivel_sector_lab',
                            'Niv_Plan_lab',
                            's_h_esc_vig',
                            'horas_pl2_pro'
                            ]
dda1_merged_max.info()

dda1_merged_cons_max = dda1_merged_cons[0:0]
#dda1_merged_cons.info()

# prompt: generar un loop  para cada grupo de dda1_merged para repetir el siguiente procedimiento dda1_merged= dda1_merged[dda1_merged['grupo'] == 1] dda1_merged ['dup_rbd7'].value_counts(dropna=False)

# Assuming dda1_merged and df_rbd_unicos are already defined as in your previous code

number_list = list(range(1, 159))
grupos=dda1_merged[dda1_merged['grupo'].isin(number_list)]

#grupos=dda1_merged[dda1_merged['grupo'].isin([1,2,3,4,5,6,7,8,9,10])]

# Group the DataFrame by 'grupo'
for group in grupos['grupo'].unique():
    # Filter the DataFrame for the current group
    dda1_merged_1 = dda1_merged[dda1_merged['grupo'] == group]

    # Perform the value counts for 'dup_rbd7' within the current group
    #print(f"Value counts for grupo {group}:")
    #print(dda1_merged_1['dup_rbd7'].value_counts(dropna=False))
   #print("-" * 20) #separator

#########################################################     Modelo horas educación general    ############################################################################################################

    # mod1: Crea h_gen1, identifica horas general
    dda1_merged_1['h_gen1'] = 0  # Inicializar la columna h_gen1 con 0
    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] == 'general':
          #Si Asig es 'general', asignar las horas_pl2 a h_gen1
          dda1_merged_1.loc[index, 'h_gen1'] = row['horas_pl2_pro']
          # mod2: modelo de asignación de horas

    # Ordenar el DataFrame por def_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('def_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'general' and row['def_vig'] < 0:
          rbd_val = row['RBD']
          #nivel_sector_val = row['nivel_sector_lab']
          Niv_Plan_lab = row['Niv_Plan_lab']

          #general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'general')]

          general_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'general')]

          for gen_index, gen_row in general_rows.iterrows():
            while dda1_merged_1.loc[gen_index, 'h_gen1'] > 0 and dda1_merged_1.loc[index, 'def_vig'] < 0:

              reduction = min(dda1_merged_1.loc[gen_index, 'h_gen1'], -dda1_merged_1.loc[index, 'def_vig'])

              dda1_merged_1.loc[gen_index, 'h_gen1'] -= reduction
              dda1_merged_1.loc[index, 'def_vig'] += reduction

   # dda1_merged_1['h_cuadre_1'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']

#####################################################         Modelo horas excedentes y hld          ##########################################################


# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
        if row['Asig'] != 'HLD' and row['sup_vig'] > 0:
            rbd_val = row['RBD']
             #nivel_sector_val = row['nivel_sector_lab']
            Niv_Plan_lab = row['Niv_Plan_lab']

          # hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) & (dda1_merged_1['Asig'] == 'HLD')]
            hld_rows = dda1_merged_1[(dda1_merged_1['RBD'] == rbd_val) & (dda1_merged_1['Niv_Plan_lab'] == Niv_Plan_lab) & (dda1_merged_1['Asig'] == 'HLD')]

            for hld_index, hld_row in hld_rows.iterrows():
              while dda1_merged_1.loc[hld_index, 'def_vig'] < 0 and dda1_merged_1.loc[index, 'sup_vig'] > 0:
                reduction = min(-dda1_merged_1.loc[hld_index, 'def_vig'], dda1_merged_1.loc[index, 'sup_vig'])

                dda1_merged_1.loc[hld_index, 'def_vig'] += reduction
                dda1_merged_1.loc[index, 'sup_vig'] -= reduction

   # dda1_merged_1['h_cuadre_2'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura dentro del nivel          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor

    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] == nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            (dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])
            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction

    #dda1_merged_1['h_cuadre_3'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


##################################################         Modelo horas excedentes misma asignatura entre niveles          ##########################################################

# prompt: asignar horas  sup_vig de cada Asig de un determinado RBD nivel_sector_lab, al def_vig de la  misma Asig - RBD- nivel_sector_lab, pero de diferente  'Niv_Plan_lab , hasta que  sup_vig=0 o def_vig=0

# Assuming dda1_merged_1 is already defined as in your provided code.

# Ordenar el DataFrame por sup_vig de mayor a menor
    dda1_merged_1 = dda1_merged_1.sort_values('sup_vig', ascending=False)

    for index, row in dda1_merged_1.iterrows():
      if row['sup_vig'] > 0:
        rbd_val = row['RBD']
        nivel_sector_val = row['nivel_sector_lab']
        asig_val = row['Asig']
        Niv_Plan_lab_val = row['Niv_Plan_lab']

        # Buscar filas con el mismo RBD, nivel_sector_lab y asignatura, pero diferente Niv_Plan_lab
        same_asig_rows = dda1_merged_1[
            (dda1_merged_1['RBD'] == rbd_val) &
            (dda1_merged_1['nivel_sector_lab'] != nivel_sector_val) &
            (dda1_merged_1['Asig'] == asig_val) &
            #(dda1_merged_1['Niv_Plan_lab'] != Niv_Plan_lab_val) &
            (dda1_merged_1['def_vig'] < 0)
        ]

        for same_asig_index, same_asig_row in same_asig_rows.iterrows():
          while dda1_merged_1.loc[index, 'sup_vig'] > 0 and dda1_merged_1.loc[same_asig_index, 'def_vig'] < 0:
            reduction = min(dda1_merged_1.loc[index, 'sup_vig'], -dda1_merged_1.loc[same_asig_index, 'def_vig'])

            dda1_merged_1.loc[index, 'sup_vig'] -= reduction
            dda1_merged_1.loc[same_asig_index, 'def_vig'] += reduction


   # dda1_merged_1['h_cuadre_4'] = dda1_merged_1['s_h_esc_vig'] + dda1_merged_1['def_vig'] + dda1_merged_1['sup_vig'] + dda1_merged_1['h_gen1'] - dda1_merged_1['horas_pl2_pro']


  # ojo con las sangrías

    dda1_merged_cons = pd.concat([dda1_merged_cons, dda1_merged_1], ignore_index=True)

"""# exportar google"""

# prompt: borrar datos de sheet "1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc" de google sheet

from google.colab import auth
auth.authenticate_user()
import gspread
from google.auth import default
creds, _ = default()
gc = gspread.authorize(creds)

# Open the Google Sheet by its key
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc")

# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different

# Clear the contents of the worksheet
worksheet.clear()

#### EXPORTA RBD ####

dfrev=dda1_merged_cons #[(dda1_agrupado['RBD'] == 25342)]

from google.colab import auth
auth.authenticate_user()

import gspread
from google.auth import default
creds, _ = default()

gc = gspread.authorize(creds)

# BORRAR HOJA
  # Open the Google Sheet by its key
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc")
# Select the worksheet you want to clear
worksheet = sheet.sheet1  # Replace 'Sheet1' with the actual worksheet name if different
# Clear the contents of the worksheet
worksheet.clear()

# ESCRIBIR HOJA
from gspread_dataframe import set_with_dataframe
sheet = gc.open_by_key("1bDKANX89w9Z7dTzVSuB9kQovyKNdsjnaA0TZ1nHPWlc").sheet1
set_with_dataframe(sheet, dfrev)